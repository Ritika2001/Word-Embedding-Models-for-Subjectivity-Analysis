{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Embedding, LSTM\n",
    "from keras.preprocessing import text,sequence\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity    10000\n",
       "text        10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Dataset/complete10000.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['text'].apply(str).values\n",
    "y = df['polarity'].values\n",
    "SEED = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19956 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.1, random_state=SEED)\n",
    "tk = text.Tokenizer(num_words=200, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True, split=\" \")\n",
    "tk.fit_on_texts(x_train)\n",
    "sequences_train = tk.texts_to_sequences(x_train)\n",
    "sequences_test = tk.texts_to_sequences(x_test)\n",
    "word_index = tk.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "x_train = pad_sequences(sequences_train, maxlen=200)\n",
    "x_test = pad_sequences(sequences_test , maxlen=200)\n",
    "x_test = pad_sequences(sequences_test , maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = tf.keras.models.load_model('./Checkpoints/Word2Vec_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 19ms/step - loss: 0.4354 - accuracy: 0.8280\n",
      "Model Loss: 0.4354 | Model Accuracy: 0.8280\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = word2vec.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = tf.keras.models.load_model('./Checkpoints/GloVe_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4893 - accuracy: 0.8420\n",
      "Model Loss: 0.4893 | Model Accuracy: 0.8420\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = glove.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = tf.keras.models.load_model('./Checkpoints/ELMO_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.6594 - acc: 0.9180\n",
      "Model Loss: 0.6594 | Model Accuracy: 0.9180\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "  DATA_COLUMN = \"text\"\n",
    "  LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "  def __init__(self, df, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_seq_len = 0\n",
    "    self.classes = classes\n",
    "    df_x, df_y = self._prepare(df)\n",
    "    SEED=2000\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_x, df_y, test_size=.1, random_state=SEED)\n",
    "\n",
    "    print(\"max seq_len\", self.max_seq_len)\n",
    "    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "    self.train_x, self.test_x = map(self._pad, [train_x, test_x])\n",
    "    self.train_y=train_y\n",
    "    self.test_y=test_y\n",
    "\n",
    "  def _prepare(self, df):\n",
    "    x, y = [], []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "      text, label = row[Preprocess.DATA_COLUMN], row[Preprocess.LABEL_COLUMN]\n",
    "      tokens = self.tokenizer.tokenize(text)\n",
    "      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "      x.append(token_ids)\n",
    "      y.append(self.classes.index(label))\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "  def _pad(self, ids):\n",
    "    x = []\n",
    "    for input_ids in ids:\n",
    "      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "      x.append(np.array(input_ids))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:03, 3000.76it/s]\n",
      "<ipython-input-5-8e670a1a6e99>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(x), np.array(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 168\n"
     ]
    }
   ],
   "source": [
    "bert_model_name=\"/home/ritika/PracticeSchool/Project/BERT/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12\"\n",
    "\n",
    "bert_ckpt_dir = bert_model_name\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"/home/ritika/PracticeSchool/Project/BERT/uncased_L-12_H-768_A-12/bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"/home/ritika/PracticeSchool/Project/BERT/uncased_L-12_H-768_A-12/bert_config.json\")\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"/home/ritika/PracticeSchool/Project/BERT/uncased_L-12_H-768_A-12/vocab.txt\"))\n",
    "\n",
    "classes = df.polarity.unique().tolist()\n",
    "\n",
    "data = Preprocess(df, tokenizer, classes, max_seq_len=128)\n",
    "\n",
    "max_seq_len = data.max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "bert_lstm = tf.keras.models.load_model('./Checkpoints/BERT_LSTM.h5', compile = False,\n",
    "                                   custom_objects={'BertModelLayer': BertModelLayer,\n",
    "                                                   'Functional':tf.keras.models.Model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lstm.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 15s 481ms/step - loss: 0.1432 - acc: 0.9650\n",
      "Model Loss: 0.1432 | Model Accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bert_lstm.evaluate(x=data.test_x, y=data.test_y, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "bert_clr = tf.keras.models.load_model('./Checkpoints/BERT_CLR.h5', compile = False,\n",
    "                                   custom_objects={'BertModelLayer': BertModelLayer,\n",
    "                                                   'Functional':tf.keras.models.Model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clr.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(0.9),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 20s 618ms/step - loss: 0.1453 - acc: 0.9610\n",
      "Model Loss: 0.1453 | Model Accuracy: 0.9610\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bert_clr.evaluate(x=data.test_x, y=data.test_y, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + CLR + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "bert_clr_lstm = tf.keras.models.load_model('./Checkpoints/BERT_LSTM_CLR.h5', compile = False,\n",
    "                                   custom_objects={'BertModelLayer': BertModelLayer,\n",
    "                                                   'Functional':tf.keras.models.Model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clr_lstm.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(0.9),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 16s 487ms/step - loss: 0.0621 - acc: 0.9790\n",
      "Model Loss: 0.0621 | Model Accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bert_clr_lstm.evaluate(x=data.test_x, y=data.test_y, verbose=1)\n",
    "print('Model Loss: {:0.4f} | Model Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
